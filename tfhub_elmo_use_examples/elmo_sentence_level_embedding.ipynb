{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# elmo_sentence_level_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example use https://tfhub.dev/google/elmo/2\n",
    "\n",
    "Text embeddings can be performed at individual word level present inside the sentence or can be performed at the entire sentence level. \n",
    "\n",
    "**This paper deals with sentence level**.\n",
    "\n",
    "```\n",
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
    "embeddings = elmo(\n",
    "[\"the cat is on the mat\", \"dogs are in the fog\"],\n",
    "signature=\"default\",\n",
    "as_dict=True)[\"elmo\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Dead work\n",
    "1. Download the data from [Kaggle's movie review sentiment analysis](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews)\n",
    "2. Unzip the downloaded files,store train.tsv and test.tsv to data file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self, params):\n",
    "        self._train_file = os.path.abspath(params['TRAIN_FILE'])\n",
    "        self._test_file = os.path.abspath(params['TEST_FILE'])\n",
    "        self._train_val_split = params['TRAIN_VAL_RATIO']\n",
    "\n",
    "        self._fetch_data()\n",
    "\n",
    "    def _fetch_data(self):\n",
    "        X_data, y_data = self._read_train_data()\n",
    "        self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(X_data, y_data,\n",
    "                                                                        train_size=self._train_val_split)\n",
    "\n",
    "        self.X_test, self.X_test_id = self._read_test_data()\n",
    "\n",
    "    def _read_train_data(self):\n",
    "        pd_data = pd.read_csv(self._train_file, sep='\\t')\n",
    "        X_data, y_data = pd_data['Phrase'], pd_data['Sentiment']\n",
    "        return X_data, y_data\n",
    "\n",
    "    def _read_test_data(self):\n",
    "        pd_data = pd.read_csv(self._test_file, sep='\\t')\n",
    "        X_data, X_data_id = pd_data['Phrase'], pd_data['PhraseId']\n",
    "        return X_data, X_data_id\n",
    "\n",
    "    def get_train_data_length(self):\n",
    "        return len(self.y_train)\n",
    "\n",
    "    def get_val_data_length(self):\n",
    "        return len(self.y_val)\n",
    "\n",
    "    def get_test_data_length(self):\n",
    "        return len(self.X_test_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, params):\n",
    "        self._n_class = params['N_CLASS']\n",
    "        self._batch_size = params['BATCH_SIZE']\n",
    "\n",
    "        self._create_iterator()\n",
    "\n",
    "    def _create_iterator(self):\n",
    "        self._pl_phrase_id = tf.placeholder(tf.int32, (None), name='pl_phrase_id')\n",
    "        self._pl_phrase_text = tf.placeholder(tf.string, (None), name='pl_phrase_text')\n",
    "        self._pl_sentiment = tf.placeholder(tf.int32, (None), name='pl_sentiment')\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((self._pl_phrase_id, self._pl_phrase_text,\n",
    "                                                      self._pl_sentiment))\n",
    "        dataset = dataset.batch(self._batch_size)\n",
    "\n",
    "        iterator = tf.data.Iterator.from_structure(dataset.output_types)\n",
    "        self._iterator_initializer = iterator.make_initializer(dataset, name='initializer')\n",
    "        self.text_id, self.text_data, self.labels = iterator.get_next()\n",
    "        self.text_id = tf.identity(self.text_id, name='text_id')\n",
    "\n",
    "    def initialize_iterator(self, sess, phrase_text, sentiment):\n",
    "        phrase_id = np.zeros((len(phrase_text)), dtype=np.int32)\n",
    "        feed_dict = {\n",
    "            self._pl_phrase_id: phrase_id,\n",
    "            self._pl_phrase_text: phrase_text,\n",
    "            self._pl_sentiment: sentiment\n",
    "        }\n",
    "        sess.run(self._iterator_initializer, feed_dict=feed_dict)\n",
    "\n",
    "    def initialize_test_iterator_for_saved_model_graph(self, sess, phrase_id, phrase_text):\n",
    "        pl_phrase_id = sess.graph.get_tensor_by_name('pl_phrase_id:0')\n",
    "        pl_phrase_text = sess.graph.get_tensor_by_name('pl_phrase_text:0')\n",
    "        pl_sentiment = sess.graph.get_tensor_by_name('pl_sentiment:0')\n",
    "        initializer = sess.graph.get_operation_by_name('initializer')\n",
    "\n",
    "        sentiment = np.zeros((len(phrase_text)), dtype=np.float32)\n",
    "        feed_dict = {\n",
    "            pl_phrase_id: phrase_id,\n",
    "            pl_phrase_text: phrase_text,\n",
    "            pl_sentiment: sentiment\n",
    "        }\n",
    "        sess.run(initializer, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, params):\n",
    "        self._n_class = params['N_CLASS']\n",
    "        self._prepare_graph(params)\n",
    "\n",
    "    def _prepare_graph(self, params):\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        self.dataset = Dataset(params)\n",
    "\n",
    "        self.lr = tf.placeholder(tf.float32, shape=())\n",
    "        one_hot_y = tf.one_hot(self.dataset.labels, depth=self._n_class)\n",
    "        logits = self._prepare_model(self.dataset.text_data)\n",
    "\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=one_hot_y)\n",
    "        self.loss = tf.reduce_sum(cross_entropy)\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(self.loss)\n",
    "\n",
    "        self.predictions = tf.argmax(logits, axis=1, output_type=tf.int32, name='predictions')\n",
    "        self.accuracy = tf.reduce_sum(tf.cast(tf.equal(self.predictions, self.dataset.labels), tf.float32))\n",
    "\n",
    "    def _prepare_model(self, text):\n",
    "        module = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
    "        embeddings = module(dict(text=text)) #shape:[batch_szie, 1024]\n",
    "        #print(embeddings)\n",
    "        logits = tf.layers.dense(embeddings, units=self._n_class)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train:\n",
    "    def __init__(self, params):\n",
    "        self._epochs = params['EPOCHS']\n",
    "        self._batch_size = params['BATCH_SIZE']\n",
    "        self._lr = params['LEARNING_RATE']\n",
    "        self._n_class = params['N_CLASS']\n",
    "\n",
    "        self.data = Data(params)\n",
    "        self.model = Model(params)\n",
    "\n",
    "        self._save_path = os.path.abspath('./Model_OUTPUT(sentence_level_elmo)')\n",
    "\n",
    "    def train(self):\n",
    "        shutil.rmtree(self._save_path, ignore_errors=True)\n",
    "        os.mkdir(self._save_path)\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "            current_lr = self._lr\n",
    "\n",
    "            for epoch_no in range(self._epochs):\n",
    "                train_loss, train_accuracy = 0, 0\n",
    "                val_loss, val_accuracy = 0, 0\n",
    "\n",
    "                print('\\nEpoch: {}, lr: {:.6f}'.format(epoch_no + 1, current_lr))\n",
    "                self.model.dataset.initialize_iterator(sess, self.data.X_train, self.data.y_train)\n",
    "                try:\n",
    "                    with tqdm(total=self.data.get_train_data_length()) as pbar:\n",
    "                        while True:\n",
    "                            _, l, a = sess.run([self.model.optimizer, self.model.loss, self.model.accuracy],\n",
    "                                               feed_dict={self.model.lr: current_lr})\n",
    "                            train_loss += l\n",
    "                            train_accuracy += a\n",
    "                            pbar.update(self._batch_size)\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "\n",
    "                self.model.dataset.initialize_iterator(sess, self.data.X_val, self.data.y_val)\n",
    "                try:\n",
    "                    with tqdm(total=self.data.get_val_data_length()) as pbar:\n",
    "                        while True:\n",
    "                            l, a = sess.run([self.model.loss, self.model.accuracy])\n",
    "                            val_loss += l\n",
    "                            val_accuracy += a\n",
    "                            pbar.update(self._batch_size)\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "\n",
    "                train_accuracy /= self.data.get_train_data_length()\n",
    "                train_loss /= self.data.get_train_data_length()\n",
    "                val_accuracy /= self.data.get_val_data_length()\n",
    "                val_loss /= self.data.get_val_data_length()\n",
    "\n",
    "                print('Train accuracy: {:.4f}, loss: {:.4f}'.format(train_accuracy, train_loss))\n",
    "                print('Validation accuracy: {:.4f}, loss: {:.4f}'.format(val_accuracy, val_loss))\n",
    "                self._save_model(sess, epoch_no)\n",
    "\n",
    "    def test(self):\n",
    "        test_graph = tf.Graph()\n",
    "        with test_graph.as_default():\n",
    "            with tf.Session(graph=test_graph) as sess, \\\n",
    "                    open(os.path.join(self._save_path, 'results.csv'), 'w') as fid:\n",
    "                sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "                saved_model_path = os.path.join(self._save_path, str(self._epochs - 1))\n",
    "                tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING], saved_model_path)\n",
    "                self.model.dataset.initialize_test_iterator_for_saved_model_graph(sess, self.data.X_test_id,\n",
    "                                                                    self.data.X_test)\n",
    "\n",
    "                csv_fid = csv.writer(fid)\n",
    "                csv_fid.writerow(['PhraseId', 'Sentiment'])\n",
    "\n",
    "                predictions_op = test_graph.get_tensor_by_name('predictions:0')\n",
    "                text_id_op = test_graph.get_tensor_by_name('text_id:0')\n",
    "                try:\n",
    "                    with tqdm(total=self.data.get_test_data_length()) as pbar:\n",
    "                        while True:\n",
    "                            predictions, phrase_id = sess.run([predictions_op, text_id_op])\n",
    "                            predictions = predictions.tolist()\n",
    "                            phrase_id = phrase_id.tolist()\n",
    "\n",
    "                            for pred, p in zip(predictions, phrase_id):\n",
    "                                csv_fid.writerow([p, pred])\n",
    "                            pbar.update(self._batch_size)\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "\n",
    "    def _save_model(self, sess, epoch_no):\n",
    "        inputs = {\n",
    "            'pl_phrase_id': sess.graph.get_tensor_by_name('pl_phrase_id:0'),\n",
    "            'pl_phrase_text': sess.graph.get_tensor_by_name('pl_phrase_text:0'),\n",
    "            'pl_sentiment': sess.graph.get_tensor_by_name('pl_sentiment:0')\n",
    "        }\n",
    "        outputs = {'accuracy': self.model.accuracy}\n",
    "\n",
    "        export_dir = os.path.join(self._save_path, str(epoch_no))\n",
    "        tf.saved_model.simple_save(sess, export_dir, inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using /tmp/tfhub_modules to cache modules.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b418/anaconda3/envs/yuanxiao/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "\n",
      "Epoch: 1, lr: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 15488/148257 [1:05:25<9:30:25,  3.88it/s] "
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'EPOCHS': 6,\n",
    "    'BATCH_SIZE': 64,\n",
    "    'LEARNING_RATE': 0.0001,\n",
    "\n",
    "    'TRAIN_FILE': './Data/train.tsv',\n",
    "    'TEST_FILE': './Data/test.tsv',\n",
    "    'TRAIN_VAL_RATIO': 0.95,\n",
    "\n",
    "    'N_CLASS': 5\n",
    "}\n",
    "\n",
    "t = Train(params)\n",
    "t.train()\n",
    "t.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
